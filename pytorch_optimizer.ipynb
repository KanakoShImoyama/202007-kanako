{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_完成版 ",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KanakoShImoyama/202007-kanako/blob/master/pytorch_optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0X54iC0bPaL"
      },
      "source": [
        "# 今後の方針\n",
        "1. algorith.pyをoptimizer フォルダに作る\n",
        "1. if \\_\\_name\\_\\_ == '\\_\\_main\\_\\_': をimdb() だけにする\n",
        "1. main.py と同じフォルダに入った状態で python main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psJxjQegRxzL"
      },
      "source": [
        "# import os\n",
        "# assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LANWybL7zIz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "029fc7ca-1732-4081-9538-d1cf3c418a9b"
      },
      "source": [
        "!pip install torch<=1.2.0\n",
        "!pip install torchtext==0.5\n",
        "\n",
        "# VERSION = \"20200325\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "# !python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: =1.2.0: No such file or directory\n",
            "Collecting torchtext==0.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5) (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5) (1.7.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 12.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.5) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.5) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.5) (0.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.5) (1.24.3)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed sentencepiece-0.1.95 torchtext-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Thgb2eZ9Y6Zt"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYYy4L4pdwtT"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "\n",
        "class SGradients(Optimizer):\n",
        "\n",
        "    def __init__(self, params, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=0, amsgrad=False, alpha=1e-3, beta=0.9, diminish=False):\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "        defaults = dict(betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay, amsgrad=amsgrad, alpha=alpha, beta=beta, diminish=diminish)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super().__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('amsgrad', False)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for x in group['params']:\n",
        "                if x.grad is None:\n",
        "                    continue\n",
        "                grad = x.grad\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "                amsgrad = group['amsgrad']\n",
        "\n",
        "                state = self.state[x]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['n'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['m'] = torch.zeros_like(x, memory_format=torch.preserve_format)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['v'] = torch.zeros_like(x, memory_format=torch.preserve_format)\n",
        "                    ### v_hatを初期化\n",
        "                    state['v_hat'] = torch.zeros_like(x, memory_format=torch.preserve_format)\n",
        "                    ### alphaを初期化\n",
        "                    state['alpha'] = group['alpha']\n",
        "                    ### betaを初期化\n",
        "                    state['beta'] = group['beta']\n",
        "\n",
        "                m, v, v_hat = state['m'], state['v'], state['v_hat']\n",
        "\n",
        "                if group['diminish'] and state['n'] > 0:\n",
        "                  state['alpha'] /= state['n'] ** 0.5\n",
        "                  try:\n",
        "                    state['beta'] /= 2 ** state['n']\n",
        "                  except OverflowError:\n",
        "                    state['beta'] = 0\n",
        "                  \n",
        "                beta_hat, beta_bar = group['betas']\n",
        "\n",
        "                if not amsgrad:\n",
        "                    bias_correction_hat = 1 - beta_hat ** (state['n'] + 1)\n",
        "                    bias_correction_bar = 1 - beta_bar ** (state['n'] + 1) \n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    grad = grad.add(x, alpha=group['weight_decay'])\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                m.mul_(state['beta']).add_(grad, alpha=1 - state['beta'])\n",
        "                ####変更部分addmul_(最初のgrad)####\n",
        "                v.mul_(beta_bar).addcmul_(grad**2 ,grad**2 , value=1 - beta_bar)\n",
        "                if amsgrad:\n",
        "                    v_bar = v\n",
        "                    torch.max(v_hat, v_bar, out=v_hat)\n",
        "                    step_size = state['alpha']\n",
        "                else:\n",
        "                    v_bar = v / bias_correction_bar\n",
        "                    torch.max(v_hat, v_bar, out=v_hat)\n",
        "                    step_size = state['alpha'] / bias_correction_hat\n",
        "                ####変更部分＃＃＃＃＃＃＃＃\n",
        "                h = v_hat.sqrt().sqrt().add_(group['eps'])\n",
        "                #h = v_hat.sqrt().add_(group['eps'])\n",
        "\n",
        "                x.addcdiv_(m, h, value=-step_size)\n",
        "\n",
        "                state['n'] += 1\n",
        "        return loss"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGgARgTzf6bg"
      },
      "source": [
        "# 新しいセクション"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioQWdlNd040L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9bfd106-3a31-4e6a-9992-8516e13d76e5"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import text_classification\n",
        "#import torch_xla.core.xla_model as xm\n",
        "\n",
        "NGRAMS = 2\n",
        "import os\n",
        "if not os.path.isdir('./.data'):\n",
        "\tos.mkdir('./.data')\n",
        "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
        "    root='./.data', ngrams=NGRAMS, vocab=None)\n",
        "BATCH_SIZE = 16\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = xm.xla_device()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ag_news_csv.tar.gz: 11.8MB [00:00, 60.9MB/s]\n",
            "120000lines [00:06, 17669.36lines/s]\n",
            "120000lines [00:15, 7675.45lines/s]\n",
            "7600lines [00:00, 7790.07lines/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVRow26qHkx9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "142f60e3-14b4-4fc4-d544-381ff06f93b0"
      },
      "source": [
        "device"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7bNKm1D06Pq"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class TextSentiment(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "        \n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXSWhLcN06R4"
      },
      "source": [
        "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
        "EMBED_DIM = 32\n",
        "NUN_CLASS = len(train_dataset.get_labels())\n",
        "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlTUDS4I06UU"
      },
      "source": [
        "def generate_batch(batch):\n",
        "    label = torch.tensor([entry[0] for entry in batch])\n",
        "    text = [entry[1] for entry in batch]\n",
        "    offsets = [0] + [len(entry) for entry in text]\n",
        "    # torch.Tensor.cumsum returns the cumulative sum\n",
        "    # of elements in the dimension dim.\n",
        "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
        "    \n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text = torch.cat(text)\n",
        "    return text, offsets, label"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lXzngRw06dG"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_func(sub_train_):\n",
        "\n",
        "    # Train the model\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                      collate_fn=generate_batch)\n",
        "    for i, (text, offsets, cls) in enumerate(data):\n",
        "        optimizer.zero_grad()\n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        output = model(text, offsets)\n",
        "        loss = criterion(output, cls)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        g_k_1 = torch.empty(1)\n",
        "        d_k_1 = np.nan\n",
        "        optimizer.step()\n",
        "        train_acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "    # Adjust the learning rate\n",
        "    # scheduler.step()\n",
        "    \n",
        "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
        "\n",
        "def test(data_):\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
        "    for text, offsets, cls in data:\n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(text, offsets)\n",
        "            loss = criterion(output, cls)\n",
        "            loss += loss.item()\n",
        "            acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "    return loss / len(data_), acc / len(data_)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9CINL8jWe7U"
      },
      "source": [
        "## 実験本体"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjdyYnkL06gS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0baa062-ff02-491f-9dfa-a6c8659a5d7a"
      },
      "source": [
        "import time\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch import optim\n",
        "\n",
        "N_EPOCHS = 200\n",
        "min_valid_loss = float('inf')\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = SGradients(model.parameters(), alpha=1e-2, beta=1e-2, diminish=False, amsgrad=True)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
        "\n",
        "train_len = int(len(train_dataset) * 0.95)\n",
        "print(\"train:\", train_len, \", valid:\", len(train_dataset) - train_len, \", test:\", len(test_dataset))\n",
        "\n",
        "sub_train_, sub_valid_ = random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
        "\n",
        "train_loss_list = [] # 保存したいもののリストを作る\n",
        "train_acc_list = []\n",
        "valid_loss_list = []\n",
        "valid_acc_list = []\n",
        "secs_list = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train_func(sub_train_)\n",
        "    valid_loss, valid_acc = test(sub_valid_)\n",
        "\n",
        "    secs = int(time.time() - start_time)\n",
        "    secs_list.append(secs) # 時間保存\n",
        "\n",
        "    mins = secs / 60\n",
        "    secs = secs % 60\n",
        "\n",
        "    train_loss_list.append(train_loss) # リストに保存する\n",
        "    train_acc_list.append(train_acc)\n",
        "    valid_loss_list.append(valid_loss)\n",
        "    valid_acc_list.append(valid_acc)\n",
        "\n",
        "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n",
        "\n",
        "result_dict = dict(\n",
        "    train_loss = train_loss_list, # 名前=リストを追加していく\n",
        "    train_acc = train_acc_list,\n",
        "    valid_loss = valid_loss_list,\n",
        "    valid_acc = valid_acc_list,\n",
        "    time=secs_list,\n",
        ")\n",
        "csv_name = 'result.csv'\n",
        "df_result = pd.DataFrame(result_dict).to_csv(csv_name, index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: 114000 , valid: 6000 , test: 7600\n",
            "Epoch: 1  | time in 2 minutes, 57 seconds\n",
            "\tLoss: 0.0178(train)\t|\tAcc: 90.7%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 92.4%(valid)\n",
            "Epoch: 2  | time in 2 minutes, 55 seconds\n",
            "\tLoss: 0.0045(train)\t|\tAcc: 97.8%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 91.8%(valid)\n",
            "Epoch: 3  | time in 2 minutes, 55 seconds\n",
            "\tLoss: 0.0014(train)\t|\tAcc: 99.4%(train)\n",
            "\tLoss: 0.0002(valid)\t|\tAcc: 91.8%(valid)\n",
            "Epoch: 4  | time in 2 minutes, 55 seconds\n",
            "\tLoss: 0.0008(train)\t|\tAcc: 99.6%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 91.9%(valid)\n",
            "Epoch: 5  | time in 2 minutes, 55 seconds\n",
            "\tLoss: 0.0005(train)\t|\tAcc: 99.7%(train)\n",
            "\tLoss: 0.0002(valid)\t|\tAcc: 92.0%(valid)\n",
            "Epoch: 6  | time in 2 minutes, 55 seconds\n",
            "\tLoss: 0.0004(train)\t|\tAcc: 99.8%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 91.9%(valid)\n",
            "Epoch: 7  | time in 2 minutes, 55 seconds\n",
            "\tLoss: 0.0004(train)\t|\tAcc: 99.8%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 92.0%(valid)\n",
            "Epoch: 8  | time in 2 minutes, 55 seconds\n",
            "\tLoss: 0.0003(train)\t|\tAcc: 99.8%(train)\n",
            "\tLoss: 0.0002(valid)\t|\tAcc: 91.8%(valid)\n",
            "Epoch: 9  | time in 2 minutes, 55 seconds\n",
            "\tLoss: 0.0003(train)\t|\tAcc: 99.9%(train)\n",
            "\tLoss: 0.0002(valid)\t|\tAcc: 92.0%(valid)\n",
            "Epoch: 10  | time in 2 minutes, 55 seconds\n",
            "\tLoss: 0.0002(train)\t|\tAcc: 99.9%(train)\n",
            "\tLoss: 0.0002(valid)\t|\tAcc: 92.0%(valid)\n",
            "Epoch: 11  | time in 2 minutes, 55 seconds\n",
            "\tLoss: 0.0002(train)\t|\tAcc: 99.9%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 91.9%(valid)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGAQ5FJxuvoW"
      },
      "source": [
        "# 新しいセクション"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c6PqLisYz99"
      },
      "source": [
        "df_result = pd.DataFrame(dict(\n",
        "    time=[10, 13, 15],\n",
        "    train_loss=[.5, .6, .3]\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4oCT02yZB9D"
      },
      "source": [
        "df_result.to_csv('result.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-OE1LAO06ms"
      },
      "source": [
        "import re\n",
        "from torchtext.data.utils import ngrams_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "ag_news_label = {1 : \"World\",\n",
        "                 2 : \"Sports\",\n",
        "                 3 : \"Business\",\n",
        "                 4 : \"Sci/Tec\"}\n",
        "\n",
        "def predict(text, model, vocab, ngrams):\n",
        "    tokenizer = get_tokenizer(\"basic_english\")\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor([vocab[token]\n",
        "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
        "    enduring the season’s worst weather conditions on Sunday at The \\\n",
        "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
        "    considering the wind and the rain was a respectable showing. \\\n",
        "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
        "    was another story. With temperatures in the mid-80s and hardly any \\\n",
        "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
        "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
        "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
        "    was even more impressive considering he’d never played the \\\n",
        "    front nine at TPC Southwind.\"\n",
        "\n",
        "vocab = train_dataset.get_vocab()\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, model, vocab, 2)])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}